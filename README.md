# Sakurako-Translate
趣味で作っている(ニューラル)翻訳モデル. 
**[うまく学習できていない]**

普通の日本語を, 
ラブライブ！サンシャイン!!の公野櫻子先生っぽい文体に変換することが目標

これを  
> なんだかキュッと。心が引き締まるような気がします。新年を迎えて初めてのお参り。うん。これってちょっぴり。プールの横の階段を登り切って飛び込み台の上に立つ時の一瞬に似てる。

こう変換したい  
> なんだかキュッと。心が引き締まるような気がします。新年を迎えて──初めてのお参り。うん♡ これってちょっぴり──。プールの横の階段を登り切って──飛び込み台の上に立つ時の一瞬に似てる。

結局のところ日本語に記号を挿入するモデルを目標としている.

<details>
<summary>使用データ</summary>

  - ラブライブ！サンシャイン!! の本文
  - 270,000文字程度
  - N = 2200程度
  
  ターゲットはこの原文で, 入力には適当なルールベースによって, 
  "──"などの記号を"。"などに変更したものを用いる.  
  データはあらかじめ分かち書きしたものを用意した.

  `data/wakati_pair.csv` がデータだが管理はしていない.

  ___
</details>

<details>
<summary>アプローチ</summary>

  ### 試したモデル

  - LSTM
  - Seq2Seq
  - Transformer

  データが少ないため, あまり表現力の高いモデル(pretrained BERTなど)は
  不適と思われる.

  また言語モデルの学習となると計算資源的な問題からも難しそう.

  ### アプローチ
  
  この変換では元の文章をほとんど変えない.

  from scratchで学習をするので, 
  ひとまず (記号なしの文章) -> (記号なしの文章) 
  の恒等射を作って(簡単), 

  (記号なしの文章) -> (記号ありの文章) の学習を試みる.

  これにつまづいている. 
  ___
</details>

## 使い方
`python(3) main.py` で学習をする.

以下のようなargumentでパラメータを指定する

|option         |内容                                                |
|:--------------|------------------------------------------------|
|-e             |エポック数                                      |
|-b             |バッチサイズ                                    |
|-d             |Dropoutの確率                                   |
|--learning_rate|学習率                                          |
|--warming_up   |学習率のschedulerのwarming upにかけるエポック数 |
|--debug        |指定するとエポック数とデータ数を減らして動作確認|
|--fine_tuning  |ターゲットを記号ありの文章にする                |

